{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering Notebook\n",
    "Introduction\n",
    "In this notebook, we will investigate the DBSCAN clustering algorithm on our dataset. Following the promising results from the Feature_Selection notebook, we will apply dimension reduction using PCA and feature selection to improve the clustering outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the environment variable\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files into DataFrames\n",
    "df_mi_corr = pd.read_csv(\"../processed_data/lazy_corr_MI_10_FS.csv\")\n",
    "df_mi_corr_50 = pd.read_csv(\"../processed_data/lazy_corr_MI_50_FS.csv\")\n",
    "df_corr = pd.read_csv(\"../processed_data/lazy_corr_FS.csv\")\n",
    "df_mi = pd.read_csv(\"../processed_data/batch_corr_MI_10_FS.csv\")\n",
    "df = pd.read_csv(\"../processed_data/batch_corr_FS.csv\")\n",
    "df_mi_50= pd.read_csv(\"../processed_data/batch_corr_MI_50_FS.csv\")\n",
    "\n",
    "\n",
    "df_corr_scaled_mi_batch_50= pd.read_csv(\"../processed_data/batch_corr_batch_MI_50_FS.csv\")\n",
    "\n",
    "df_corr_scaled_nomiddle= pd.read_csv(\"../processed_data/batch_corr_MI_nomiddle_50_FS.csv\")\n",
    "\n",
    "df_corr_scaled_mi_nofreq_50= pd.read_csv(\"../processed_data/batch_corr_MI_nofreq_50_FS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods\n",
    "One of the hard thing about the clustering is how to evaluate the clustering. To do so we will leverage the nature of our dataset.\n",
    "\n",
    "weighted average of average variability of pingtime: This will reveals the how much variability within the cluster. This does not compare each cluster's quality and having more cluster is always better but this doesn't account that either.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "\n",
    "def average_variability_metrics(df_cluster, all_cleaned_df):\n",
    "    \"\"\"\n",
    "    Calculate the average number of outliers and average standard deviation of ping time for each cluster,\n",
    "    and the weighted averages of these values.\n",
    "\n",
    "    Parameters:\n",
    "    df_cluster (DataFrame): The DataFrame containing sensor ID and their respective clusters.\n",
    "    all_cleaned_df (DataFrame): The DataFrame containing the cleaned data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with columns for cluster, average number of outliers, and average std ping time.\n",
    "    float: The weighted average of the average number of outliers across clusters.\n",
    "    float: The weighted average of the average standard deviation of ping time across clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each cluster\n",
    "    for target in df_cluster[\"cluster\"].unique():\n",
    "        # Get the sensor IDs for the current cluster\n",
    "        cluster_sensors = df_cluster[df_cluster[\"cluster\"] == target][\"Sensor ID\"].unique()\n",
    "\n",
    "        # Filter the all_cleaned_df for the current cluster sensors\n",
    "        all_cleaned_df_target = all_cleaned_df[all_cleaned_df['Sensor ID'].isin(cluster_sensors)]\n",
    "\n",
    "        # Group by 'Delay (us)' and 'Range (cm)', then calculate the number of outliers in 'Ping Time (us)'\n",
    "        grouped_outliers = all_cleaned_df_target.groupby(['Delay (us)', 'Range (cm)'])['Ping Time (us)'].apply(identify_outliers).reset_index(name='outliers')\n",
    "        \n",
    "        # Calculate the average number of outliers for the current cluster\n",
    "        avg_count_outliers = grouped_outliers['outliers'].mean()\n",
    "        max_count_outliers = grouped_outliers['outliers'].max()\n",
    "\n",
    "        # Group by sensor ID, delay, and range, then calculate the standard deviation of ping time\n",
    "        grouped_std = all_cleaned_df_target.groupby(['Sensor ID', 'Delay (us)', 'Range (cm)']).agg(\n",
    "            std_ping_time=('Ping Time (us)', 'std')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate the average std ping time for the current cluster\n",
    "        avg_std_ping_time = grouped_std['std_ping_time'].mean()\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({'cluster': target, 'max_count_outliers':max_count_outliers, 'avg_count_outliers': avg_count_outliers, 'avg_std_ping_time': avg_std_ping_time, 'count': len(cluster_sensors)})\n",
    "    \n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate the weighted average of the average number of outliers\n",
    "\n",
    "    results_df['weighted_avg_count_outliers'] =(results_df['max_count_outliers']-results_df['avg_count_outliers']) /results_df['count']\n",
    "    weighted_avg_count_outliers_score = results_df['weighted_avg_count_outliers'].mean()\n",
    "\n",
    "    # Calculate the weighted average of the average std ping time\n",
    "    results_df['weighted_avg_std_ping_time'] = results_df['avg_std_ping_time'] /results_df['count']*(results_df['cluster']+1)\n",
    "    weighted_avg_std_ping_time_score = results_df['weighted_avg_std_ping_time'].mean()\n",
    "\n",
    "    return results_df, weighted_avg_count_outliers_score, weighted_avg_std_ping_time_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Investigate Clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lineplot_ping_time_with_variability(df, target = []):\n",
    "    \"\"\"\n",
    "    Visualize the effect of range on ping time for each delay separately with variability.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    # Group by sensor ID, delay, and range, then calculate the mean and standard deviation of ping time\n",
    "    grouped_df = df.groupby(['Sensor ID', 'Delay (us)', 'Range (cm)']).agg(\n",
    "        mean_ping_time=('Ping Time (us)', 'mean'),\n",
    "        std_ping_time=('Ping Time (us)', 'std')\n",
    "    ).reset_index()\n",
    "\n",
    "    target_df = grouped_df[grouped_df['Sensor ID'].isin(target)]\n",
    "    # Get unique delays\n",
    "    unique_delays = target_df['Delay (us)'].unique()\n",
    "\n",
    "    for delay in unique_delays:\n",
    "        subset_df = target_df[target_df['Delay (us)'] == delay]\n",
    "\n",
    "        fig = px.line(\n",
    "\n",
    "        )\n",
    "\n",
    "        # Adding error bars\n",
    "        for sensor_id in subset_df['Sensor ID'].unique():\n",
    "            sensor_data = subset_df[subset_df['Sensor ID'] == sensor_id]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sensor_data['Range (cm)'],\n",
    "                    y=sensor_data['mean_ping_time'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'Sensor {sensor_id}',\n",
    "                    error_y=dict(\n",
    "                        type='data',\n",
    "                        array=sensor_data['std_ping_time'],\n",
    "                        visible=True\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot reference line\n",
    "        ranges = np.linspace(subset_df['Range (cm)'].min(), subset_df['Range (cm)'].max(), 100)\n",
    "        reference_ping_times = 57 * ranges\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ranges,\n",
    "                y=reference_ping_times,\n",
    "                mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name='Reference Line'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Range (cm)',\n",
    "            yaxis_title='Mean Ping Time (us)',\n",
    "            legend_title='Sensor ID',\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "def visualize_lineplot_ping_time_with_variability_simple(df, target=[]):\n",
    "    \"\"\"\n",
    "    Visualize the effect of range on ping time for each delay separately with variability.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target (list): List of target sensor IDs to visualize.\n",
    "    \"\"\"\n",
    "    # Group by sensor ID, delay, and range, then calculate the mean and standard deviation of ping time\n",
    "    grouped_df = df.groupby(['Sensor ID', 'Delay (us)', 'Range (cm)']).agg(\n",
    "        mean_ping_time=('Ping Time (us)', 'mean'),\n",
    "        std_ping_time=('Ping Time (us)', 'std')\n",
    "    ).reset_index()\n",
    "\n",
    "    target_df = grouped_df[grouped_df['Sensor ID'].isin(target)]\n",
    "    # Get unique delays\n",
    "    unique_delays = target_df['Delay (us)'].unique()\n",
    "\n",
    "    # Define the number of columns for subplots\n",
    "    num_columns = 2  # 2 columns for the grid\n",
    "    num_rows = 3  # 3 rows for the grid\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_columns, subplot_titles=[f'Delay: {delay} us' for delay in unique_delays])\n",
    "\n",
    "    for i, delay in enumerate(unique_delays):\n",
    "        subset_df = target_df[target_df['Delay (us)'] == delay]\n",
    "\n",
    "        # Get the row and column position for the subplot\n",
    "        row = (i // num_columns) + 1\n",
    "        col = (i % num_columns) + 1\n",
    "\n",
    "        # Adding error bars\n",
    "        for sensor_id in subset_df['Sensor ID'].unique():\n",
    "            sensor_data = subset_df[subset_df['Sensor ID'] == sensor_id]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sensor_data['Range (cm)'],\n",
    "                    y=sensor_data['mean_ping_time'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'Sensor {sensor_id}',\n",
    "                    error_y=dict(\n",
    "                        type='data',\n",
    "                        array=sensor_data['std_ping_time'],\n",
    "                        visible=True\n",
    "                    )\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "        # Plot reference line\n",
    "        ranges = np.linspace(subset_df['Range (cm)'].min(), subset_df['Range (cm)'].max(), 100)\n",
    "        reference_ping_times = 57 * ranges\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ranges,\n",
    "                y=reference_ping_times,\n",
    "                mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name='Reference Line'\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1500,  # Adjust height for the grid layout\n",
    "        width=1500,  # Adjust width for the grid layout\n",
    "        showlegend=False,\n",
    "        title_text=\"Ping Time vs Range for Different Delays\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def visualize_cluster(df,cluster = 0, simple = True):\n",
    "    # Load the dataset\n",
    "    file_path = '../processed_data/all_data_v4-1-1_cleaned_sensor211.csv'\n",
    "    all_cleaned_df = pd.read_csv(file_path)\n",
    "    all_cleaned_df = all_cleaned_df.drop(\"Unnamed: 0\",axis=1)\n",
    "    \n",
    "    cluster_sensors = df[df[\"cluster\"]==cluster][\"Sensor ID\"].unique()\n",
    "    if simple:\n",
    "        visualize_lineplot_ping_time_with_variability_simple(all_cleaned_df,cluster_sensors)\n",
    "    else:\n",
    "        visualize_lineplot_ping_time_with_variability(all_cleaned_df,cluster_sensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '../processed_data/all_data_v4-1-1_cleaned_sensor211.csv'\n",
    "all_cleaned_df = pd.read_csv(file_path)\n",
    "all_cleaned_df = all_cleaned_df.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_visualize_dbscan(data, eps_range, min_samples_range, plot_3d=False):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for DBSCAN clustering,\n",
    "    store the results in a DataFrame, visualize the results, and return the best model.\n",
    "    Optionally, create an interactive 3D plot of the clustering results.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame - The input data for DBSCAN clustering.\n",
    "    - eps_range: list or array - The range of epsilon values to try.\n",
    "    - min_samples_range: list or array - The range of min_samples values to try.\n",
    "    - plot_3d: bool - Whether to create an interactive 3D plot of the clustering results.\n",
    "\n",
    "    Returns:\n",
    "    - best_model: DBSCAN - The best DBSCAN model based on the silhouette score.\n",
    "    - results_df: pd.DataFrame - The DataFrame containing silhouette scores for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop the target column if it's present\n",
    "    if 'target' in data.columns:\n",
    "        data = data.drop('target', axis=1)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # Create lists to store the results\n",
    "    eps_values = []\n",
    "    min_samples_values = []\n",
    "    silhouette_scores = []\n",
    "    n_clusters_list = []\n",
    "\n",
    "    # Loop over all combinations of eps and min_samples\n",
    "    for eps, min_samples in product(eps_range, min_samples_range):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(features_scaled)\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters < 2:\n",
    "            # Silhouette score cannot be computed with less than 2 clusters\n",
    "            silhouette = np.nan\n",
    "        else:\n",
    "            # Exclude noise points for silhouette score\n",
    "            mask = labels != -1\n",
    "            silhouette = silhouette_score(features_scaled[mask], labels[mask])\n",
    "\n",
    "        eps_values.append(eps)\n",
    "        min_samples_values.append(min_samples)\n",
    "        silhouette_scores.append(silhouette)\n",
    "        n_clusters_list.append(n_clusters)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'eps': eps_values,\n",
    "        'min_samples': min_samples_values,\n",
    "        'silhouette_score': silhouette_scores,\n",
    "        'n_clusters': n_clusters_list\n",
    "    })\n",
    "\n",
    "    # Remove rows with NaN silhouette scores (due to less than 2 clusters)\n",
    "    valid_results_df = results_df.dropna(subset=['silhouette_score'])\n",
    "\n",
    "    if valid_results_df.empty:\n",
    "        print(\"No valid clustering found with the given parameter ranges.\")\n",
    "        print(\"Consider adjusting the eps and min_samples ranges.\")\n",
    "        # Optionally, you can return the results_df for further analysis\n",
    "        return None, results_df\n",
    "\n",
    "    # Find the best model based on the highest silhouette score\n",
    "    best_index = valid_results_df['silhouette_score'].idxmax()\n",
    "    best_eps = valid_results_df.loc[best_index, 'eps']\n",
    "    best_min_samples = valid_results_df.loc[best_index, 'min_samples']\n",
    "    best_model = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "    best_model.fit(features_scaled)\n",
    "    best_labels = best_model.labels_\n",
    "\n",
    "    # Plot the silhouette scores as a heatmap\n",
    "    pivot_table = valid_results_df.pivot('min_samples', 'eps', 'silhouette_score')\n",
    "    fig = px.imshow(pivot_table, \n",
    "                    x=pivot_table.columns, \n",
    "                    y=pivot_table.index, \n",
    "                    color_continuous_scale='Viridis',\n",
    "                    labels={'x': 'Epsilon', 'y': 'Min Samples', 'color': 'Silhouette Score'},\n",
    "                    title='Silhouette Scores for different combinations of eps and min_samples')\n",
    "    fig.show()\n",
    "\n",
    "    # Optionally, create an interactive 3D plot of the clustering results\n",
    "    if plot_3d:\n",
    "        if features_scaled.shape[1] < 3:\n",
    "            raise ValueError(\"The dataset must have at least 3 features for a 3D plot.\")\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        data_plot = pd.DataFrame(features_scaled[:, :3], columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "        data_plot['cluster'] = best_labels.astype(str)\n",
    "        data_plot['cluster'] = data_plot['cluster'].replace({'-1': 'Noise'})\n",
    "\n",
    "        # Create 3D scatter plot\n",
    "        fig_3d = px.scatter_3d(\n",
    "            data_plot, \n",
    "            x='Feature 1', \n",
    "            y='Feature 2', \n",
    "            z='Feature 3', \n",
    "            color='cluster',\n",
    "            title='DBSCAN Clustering Results in 3D',\n",
    "            labels={'cluster': 'Cluster'}\n",
    "        )\n",
    "        fig_3d.show()\n",
    "\n",
    "    return best_model, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DBSCAN(df, eps=0.5, min_samples=5, visualization_method='PCA', plot_3d=False):\n",
    "    \"\"\"\n",
    "    Train a DBSCAN model on the given dataframe, predict clusters, and visualize the results.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the features to cluster.\n",
    "    eps (float): The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    visualization_method (str): The method for visualization ('PCA' or 'TSNE').\n",
    "    plot_3d (bool): Whether to generate a 3D plot. If False, a 2D plot will be generated.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The original DataFrame with an additional column for cluster labels.\n",
    "    DBSCAN: The fitted DBSCAN model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the features\n",
    "    df = df.copy()\n",
    "    sensor_ids = df.index if 'Sensor ID' not in df.columns else df['Sensor ID']\n",
    "    features = df.drop(columns=['Sensor ID']) if 'Sensor ID' in df.columns else df\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Fit a DBSCAN model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(features_scaled)\n",
    "\n",
    "    # Predict cluster labels\n",
    "    cluster_labels = dbscan.labels_\n",
    "    df['cluster'] = cluster_labels\n",
    "\n",
    "    print(\"============ Distribution of Sensors in each Cluster ============\")\n",
    "    print(df.groupby([\"cluster\"])[\"Sensor ID\"].count() if 'Sensor ID' in df.columns else df['cluster'].value_counts())\n",
    "\n",
    "    # Evaluate the model using silhouette score\n",
    "    # Silhouette score requires at least 2 clusters (excluding noise)\n",
    "    labels_unique = np.unique(cluster_labels)\n",
    "    n_clusters = len(labels_unique) - (1 if -1 in cluster_labels else 0)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        # Exclude noise points for silhouette score\n",
    "        mask = cluster_labels != -1\n",
    "        silhouette_avg = silhouette_score(features_scaled[mask], cluster_labels[mask])\n",
    "        print(f\"Silhouette Score (excluding noise): {silhouette_avg:.4f}\")\n",
    "    else:\n",
    "        print(\"Silhouette Score cannot be computed with less than 2 clusters (excluding noise).\")\n",
    "    \n",
    "    # Visualize the clustering results using PCA or t-SNE\n",
    "    if visualization_method.upper() == 'PCA':\n",
    "        n_components = 3 if plot_3d else 2\n",
    "        pca = PCA(n_components=n_components)\n",
    "        components = pca.fit_transform(features_scaled)\n",
    "        title = '3D Visualization using PCA' if plot_3d else '2D Visualization using PCA'\n",
    "    elif visualization_method.upper() == 'TSNE':\n",
    "        n_components = 3 if plot_3d else 2\n",
    "        tsne = TSNE(n_components=n_components, random_state=42)\n",
    "        components = tsne.fit_transform(features_scaled)\n",
    "        title = '3D Visualization using t-SNE' if plot_3d else '2D Visualization using t-SNE'\n",
    "    else:\n",
    "        raise ValueError(\"visualization_method should be either 'PCA' or 'TSNE'.\")\n",
    "\n",
    "    # Create a DataFrame for the components\n",
    "    components_df = pd.DataFrame(components, columns=[f'Component {i+1}' for i in range(n_components)])\n",
    "    components_df['cluster'] = cluster_labels.astype(str)  # Convert to string for better coloring\n",
    "    components_df['Sensor ID'] = sensor_ids\n",
    "\n",
    "    # Handle noise points in visualization\n",
    "    components_df['cluster'] = components_df['cluster'].replace({'-1': 'Noise'})\n",
    "\n",
    "    # Plot the results\n",
    "    if plot_3d:\n",
    "        fig = px.scatter_3d(\n",
    "            components_df, \n",
    "            x='Component 1', \n",
    "            y='Component 2', \n",
    "            z='Component 3', \n",
    "            color='cluster', \n",
    "            title=title,\n",
    "            hover_name='Sensor ID',  # Display Sensor ID on hover\n",
    "            color_discrete_sequence=px.colors.qualitative.G10  # Set color palette\n",
    "        )\n",
    "    else:\n",
    "        fig = px.scatter(\n",
    "            components_df, \n",
    "            x='Component 1', \n",
    "            y='Component 2', \n",
    "            color='cluster', \n",
    "            title=title,\n",
    "            hover_name='Sensor ID',  # Display Sensor ID on hover\n",
    "            color_discrete_sequence=px.colors.qualitative.G10  # Set color palette\n",
    "        )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    return df, dbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dbscan_weighted_avg(df, data, eps_range=np.linspace(0.1, 1.0, 10), min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform clustering using DBSCAN over a range of eps values, and compute weighted average metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing features to cluster.\n",
    "    - data: Additional data used in average_variability_metrics function.\n",
    "    - eps_range: Array-like of eps values to try.\n",
    "    - min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "    Returns:\n",
    "    - None. Prints out the weighted average metrics for each eps value.\n",
    "    \"\"\"\n",
    "    for eps in eps_range:\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(df)\n",
    "\n",
    "        # Fit a DBSCAN model\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan.fit(features_scaled)\n",
    "\n",
    "        # Get cluster labels\n",
    "        cluster_labels = dbscan.labels_\n",
    "        df['cluster'] = cluster_labels\n",
    "\n",
    "        # Compute the metrics\n",
    "        results_df, weighted_avg_outliers_score, weighted_avg_std_ping_time_score = average_variability_metrics(df, data)\n",
    "\n",
    "        # Get number of clusters (excluding noise)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "\n",
    "        # Print the weighted averages\n",
    "        print(f\"Weighted avg variability score when eps={eps:.2f}, n_clusters={n_clusters}: {weighted_avg_std_ping_time_score}, Outlier score: {weighted_avg_outliers_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate each feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature set with no freq and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.pivot() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m min_samples_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m)       \u001b[38;5;66;03m# Keep min_samples range reasonable\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m best_model, results_df \u001b[38;5;241m=\u001b[39m tune_and_visualize_dbscan(df_corr_scaled_mi_nofreq_50, eps_range, min_samples_range, plot_3d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [15], line 78\u001b[0m, in \u001b[0;36mtune_and_visualize_dbscan\u001b[1;34m(data, eps_range, min_samples_range, plot_3d)\u001b[0m\n\u001b[0;32m     75\u001b[0m best_labels \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Plot the silhouette scores as a heatmap\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m pivot_table \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_results_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_samples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msilhouette_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mimshow(pivot_table, \n\u001b[0;32m     80\u001b[0m                 x\u001b[38;5;241m=\u001b[39mpivot_table\u001b[38;5;241m.\u001b[39mcolumns, \n\u001b[0;32m     81\u001b[0m                 y\u001b[38;5;241m=\u001b[39mpivot_table\u001b[38;5;241m.\u001b[39mindex, \n\u001b[0;32m     82\u001b[0m                 color_continuous_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViridis\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     83\u001b[0m                 labels\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpsilon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin Samples\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSilhouette Score\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m     84\u001b[0m                 title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSilhouette Scores for different combinations of eps and min_samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.pivot() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "eps_range = np.linspace(0.1, 5.0, 50)  # Adjust the upper limit as needed\n",
    "min_samples_range = range(2, 10)       # Keep min_samples range reasonable\n",
    "\n",
    "# Call the function\n",
    "best_model, results_df = tune_and_visualize_dbscan(df_corr_scaled_mi_nofreq_50, eps_range, min_samples_range, plot_3d=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
